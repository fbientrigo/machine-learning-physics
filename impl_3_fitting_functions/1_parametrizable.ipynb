{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import basic libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Import PyTorch\n",
    "import torch # import main library\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn # import modules\n",
    "from torch.autograd import Function # import Function to create custom activations\n",
    "from torch.nn.parameter import Parameter # import Parameter to create custom activations with learnable parameters\n",
    "from torch import optim # import optimizers for demonstrations\n",
    "import torch.nn.functional as F # import torch functions\n",
    "from torchvision import datasets, transforms # import transformations to use for demo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# helper function to train a model\n",
    "def train_model(model, trainloader, epochs = 5):\n",
    "    '''\n",
    "    Function trains the model and prints out the training log.\n",
    "    INPUT:\n",
    "        model - initialized PyTorch model ready for training.\n",
    "        trainloader - PyTorch dataloader for training data.\n",
    "    '''\n",
    "    #setup training\n",
    "\n",
    "    #define loss function\n",
    "    criterion = nn.NLLLoss()\n",
    "    #define learning rate\n",
    "    learning_rate = 0.003\n",
    "    \n",
    "    #initialize optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    #run training and print out the loss to make sure that we are actually fitting to the training set\n",
    "    print('Training the model. Make sure that loss decreases after each epoch.\\n')\n",
    "    for e in range(epochs):\n",
    "        running_loss = 0\n",
    "        for images, labels in trainloader:\n",
    "            images = images.view(images.shape[0], -1)\n",
    "            log_ps = model(images)\n",
    "            loss = criterion(log_ps, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        else:\n",
    "            # print out the loss to make sure it is decreasing\n",
    "            print(f\"Training loss: {running_loss}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametrizable Activation Functions\n",
    "We wish to create new activation functions that are trainable\n",
    "\n",
    "Read more on:\n",
    "- Bingham, G., & Miikkulainen, R. (2020). Discovering Parametric Activation Functions. ArXiv.org. https://arxiv.org/abs/2006.03179v4\n",
    "- Deis, A. (2019, June 27). Extending PyTorch with Custom Activation Functions - Towards Data Science. Medium; Towards Data Science. https://towardsdatascience.com/extending-pytorch-with-custom-activation-functions-2d8b065ef2fa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Adding a new soft exponential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\msys64\\home\\Code\\ML-physics\\impl_3_fitting_functions\\1_parametrizable.ipynb Cell 3\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/msys64/home/Code/ML-physics/impl_3_fitting_functions/1_parametrizable.ipynb#W0sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39msoft_exponential\u001b[39;00m(nn\u001b[39m.\u001b[39mModule):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/msys64/home/Code/ML-physics/impl_3_fitting_functions/1_parametrizable.ipynb#W0sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/msys64/home/Code/ML-physics/impl_3_fitting_functions/1_parametrizable.ipynb#W0sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m    Implementation of soft exponential activation.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/msys64/home/Code/ML-physics/impl_3_fitting_functions/1_parametrizable.ipynb#W0sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m    code based on Deis, A. (2019)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/msys64/home/Code/ML-physics/impl_3_fitting_functions/1_parametrizable.ipynb#W0sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m        >>> x = a1(x)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/msys64/home/Code/ML-physics/impl_3_fitting_functions/1_parametrizable.ipynb#W0sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/msys64/home/Code/ML-physics/impl_3_fitting_functions/1_parametrizable.ipynb#W0sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, in_features, alpha \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "class soft_exponential(nn.Module):\n",
    "    '''\n",
    "    Implementation of soft exponential activation.\n",
    "    code based on Deis, A. (2019)\n",
    "    Shape:\n",
    "        - Input: (N, *) where * means, any number of additional\n",
    "          dimensions\n",
    "        - Output: (N, *), same shape as the input\n",
    "    Parameters:\n",
    "        - alpha - trainable parameter\n",
    "    References:\n",
    "        - See related paper:\n",
    "        https://arxiv.org/pdf/1602.01321.pdf\n",
    "    Examples:\n",
    "        >>> a1 = soft_exponential(256)\n",
    "        >>> x = torch.randn(256)\n",
    "        >>> x = a1(x)\n",
    "    '''\n",
    "    def __init__(self, in_features, alpha = None):\n",
    "        '''\n",
    "        Initialization.\n",
    "        INPUT:\n",
    "            - in_features: shape of the input\n",
    "            - aplha: trainable parameter\n",
    "            aplha is initialized with zero value by default\n",
    "        '''\n",
    "        super(soft_exponential,self).__init__()\n",
    "        self.in_features = in_features\n",
    "\n",
    "        # initialize alpha\n",
    "        if alpha == None:\n",
    "            self.alpha = Parameter(torch.tensor(0.0)) # create a tensor out of alpha\n",
    "        else:\n",
    "            self.alpha = Parameter(torch.tensor(alpha)) # create a tensor out of alpha\n",
    "            \n",
    "        self.alpha.requiresGrad = True # set requiresGrad to true!\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Forward pass of the function.\n",
    "        Applies the function to the input elementwise.\n",
    "        '''\n",
    "        if (self.alpha == 0.0):\n",
    "            return x\n",
    "\n",
    "        if (self.alpha < 0.0):\n",
    "            return - torch.log(1 - self.alpha * (x + self.alpha)) / self.alpha\n",
    "\n",
    "        if (self.alpha > 0.0):\n",
    "            return (torch.exp(self.alpha * x) - 1)/ self.alpha + self.alpha\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
