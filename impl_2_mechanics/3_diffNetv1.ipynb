{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# =================================================================\n",
    "\n",
    "N = 256 # size of the array of Function F\n",
    "\n",
    "# =================================================================\n",
    "\n",
    "# Activation function for fitting\n",
    "class F_function(nn.Module):\n",
    "    \"\"\"\n",
    "    Activation function for fitting, works by saving points in a parametrized array\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(F_function, self).__init__()\n",
    "        self.force = nn.Parameter(torch.rand(N+1), requires_grad=True)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        does an interpolation, in theory takes the input\n",
    "        int i = v;\n",
    "        and tries to do\n",
    "        return F[v] \n",
    "        to find the corresponding value to that ´v´\n",
    "        \"\"\"\n",
    "\n",
    "        if len(X) == 1:\n",
    "            x, v = X[0], X[1]\n",
    "        else:\n",
    "            x, v = X[:,0], X[:,1]\n",
    "\n",
    "        floor_v = torch.floor(v)\n",
    "        ceil_v = (floor_v + 1).clamp(max=N) #adresses the overflow problem\n",
    "        alpha = v - floor_v\n",
    "\n",
    "\n",
    "        # return torch.tensor([x, (1 - alpha) * self.force[floor_v.int()] + alpha * self.force[ceil_v.int()]])\n",
    "        return torch.stack([x, (1 - alpha) * self.force[floor_v.int()] + alpha * self.force[ceil_v.int()]]).T\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## constructing a network with this component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# =================================================================\n",
    "\n",
    "N = 256 # size of the array of Function F\n",
    "\n",
    "# =================================================================    \n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class W_matrix(nn.Module):\n",
    "    \"\"\" we need a linear thats not trainable \"\"\"\n",
    "    def __init__(self, dt):\n",
    "        super(W_matrix, self).__init__()\n",
    "        self.weights = torch.Tensor([[1, dt], [0, 1]])\n",
    "        self.bias = torch.Tensor([0, 0])\n",
    "        \n",
    "    def forward(self, x): \n",
    "        return F.linear(x, self.weights, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class diffNet(nn.Module):\n",
    "    def __init__(self, depth):\n",
    "        super(diffNet, self).__init__()\n",
    "        layers = []\n",
    "        layers.append(W_matrix(dt=1e-02))\n",
    "        for i in range(depth):\n",
    "            layers.append( F_function() )\n",
    "            layers.append( W_matrix(dt=1e-02) )\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating the data from the simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inicial</th>\n",
       "      <th>final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>101.863643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.185320</td>\n",
       "      <td>102.051319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.370867</td>\n",
       "      <td>102.239240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.556643</td>\n",
       "      <td>102.427406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.742649</td>\n",
       "      <td>102.615821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      inicial       final\n",
       "0  100.000000  101.863643\n",
       "1  100.185320  102.051319\n",
       "2  100.370867  102.239240\n",
       "3  100.556643  102.427406\n",
       "4  100.742649  102.615821"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('.\\data\\songforce_v100\\songforce.csv')\n",
    "df.head()\n",
    "\n",
    "data_worked = pd.DataFrame()\n",
    "inicial = []\n",
    "final = []\n",
    "\n",
    "for index in range(10,len(df)-1):\n",
    "    final.append(df.velocity[index])\n",
    "    inicial.append(df.velocity[index - 10])\n",
    "\n",
    "# print(inicial)\n",
    "data_worked['inicial'] = inicial\n",
    "data_worked['final'] = final\n",
    "\n",
    "# seoarate \n",
    "Ndata = len(data_worked)\n",
    "datatraining = data_worked[:202]\n",
    "datavalidation = data_worked[202:250]\n",
    "datatesting = data_worked[250:-1]\n",
    "\n",
    "data_worked.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([202, 2])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = torch.tensor( [ [0 for i in range(len(datatraining))], datatraining.inicial  ] ,  dtype=torch.float32).T\n",
    "Y = torch.tensor( [ [0 for i in range(len(datatraining))], datatraining.final  ] ,  dtype=torch.float32)\n",
    "\n",
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0451, 0.5921],\n",
       "        [1.0477, 0.5921],\n",
       "        [1.0504, 0.5921],\n",
       "        [1.0530, 0.5921],\n",
       "        [1.0557, 0.5921],\n",
       "        [1.0584, 0.5921],\n",
       "        [1.0603, 0.5921],\n",
       "        [1.0619, 0.5921],\n",
       "        [1.0634, 0.5921],\n",
       "        [1.0650, 0.5921],\n",
       "        [1.0665, 0.5921],\n",
       "        [1.0684, 0.5921],\n",
       "        [1.0711, 0.5921],\n",
       "        [1.0738, 0.5921],\n",
       "        [1.0765, 0.5921],\n",
       "        [1.0792, 0.5921],\n",
       "        [1.0819, 0.5921],\n",
       "        [1.0835, 0.5921],\n",
       "        [1.0850, 0.5921],\n",
       "        [1.0865, 0.5921],\n",
       "        [1.0880, 0.5921],\n",
       "        [1.0895, 0.5921],\n",
       "        [1.0916, 0.5921],\n",
       "        [1.0939, 0.5921],\n",
       "        [1.0963, 0.5921],\n",
       "        [1.0986, 0.5921],\n",
       "        [1.1010, 0.5921],\n",
       "        [1.1030, 0.5921],\n",
       "        [1.1045, 0.5921],\n",
       "        [1.1061, 0.5921],\n",
       "        [1.1077, 0.5921],\n",
       "        [1.1092, 0.5921],\n",
       "        [1.1108, 0.5921],\n",
       "        [1.1122, 0.5921],\n",
       "        [1.1136, 0.5921],\n",
       "        [1.1151, 0.5921],\n",
       "        [1.1165, 0.5921],\n",
       "        [1.1180, 0.5921],\n",
       "        [1.1201, 0.5921],\n",
       "        [1.1222, 0.5921],\n",
       "        [1.1242, 0.5921],\n",
       "        [1.1263, 0.5921],\n",
       "        [1.1283, 0.5921],\n",
       "        [1.1295, 0.5921],\n",
       "        [1.1306, 0.5921],\n",
       "        [1.1318, 0.5921],\n",
       "        [1.1329, 0.5921],\n",
       "        [1.1341, 0.5921],\n",
       "        [1.1372, 0.5921],\n",
       "        [1.1404, 0.5921],\n",
       "        [1.1436, 0.5921],\n",
       "        [1.1468, 0.5921],\n",
       "        [1.1501, 0.5921],\n",
       "        [1.1523, 0.5921],\n",
       "        [1.1544, 0.5921],\n",
       "        [1.1565, 0.5921],\n",
       "        [1.1586, 0.5921],\n",
       "        [1.1607, 0.5921],\n",
       "        [1.1625, 0.5921],\n",
       "        [1.1643, 0.5921],\n",
       "        [1.1660, 0.5921],\n",
       "        [1.1678, 0.5921],\n",
       "        [1.1695, 0.5921],\n",
       "        [1.1717, 0.5921],\n",
       "        [1.1738, 0.5921],\n",
       "        [1.1759, 0.5921],\n",
       "        [1.1781, 0.5921],\n",
       "        [1.1802, 0.5921],\n",
       "        [1.1813, 0.5921],\n",
       "        [1.1824, 0.5921],\n",
       "        [1.1835, 0.5921],\n",
       "        [1.1847, 0.5921],\n",
       "        [1.1859, 0.5921],\n",
       "        [1.1876, 0.5921],\n",
       "        [1.1892, 0.5921],\n",
       "        [1.1909, 0.5921],\n",
       "        [1.1925, 0.5921],\n",
       "        [1.1944, 0.5921],\n",
       "        [1.1967, 0.5921],\n",
       "        [1.1989, 0.5921],\n",
       "        [1.2012, 0.5921],\n",
       "        [1.2034, 0.5921],\n",
       "        [1.2056, 0.5921],\n",
       "        [1.2078, 0.5921],\n",
       "        [1.2099, 0.5921],\n",
       "        [1.2121, 0.5921],\n",
       "        [1.2142, 0.5921],\n",
       "        [1.2173, 0.5921],\n",
       "        [1.2206, 0.5921],\n",
       "        [1.2239, 0.5921],\n",
       "        [1.2272, 0.5921],\n",
       "        [1.2303, 0.5921],\n",
       "        [1.2319, 0.5921],\n",
       "        [1.2336, 0.5921],\n",
       "        [1.2352, 0.5921],\n",
       "        [1.2369, 0.5921],\n",
       "        [1.2391, 0.5921],\n",
       "        [1.2418, 0.5921],\n",
       "        [1.2445, 0.5921],\n",
       "        [1.2472, 0.5921],\n",
       "        [1.2499, 0.5921],\n",
       "        [1.2512, 0.5921],\n",
       "        [1.2525, 0.5921],\n",
       "        [1.2537, 0.5921],\n",
       "        [1.2550, 0.5921],\n",
       "        [1.2569, 0.5921],\n",
       "        [1.2599, 0.5921],\n",
       "        [1.2629, 0.5921],\n",
       "        [1.2659, 0.5921],\n",
       "        [1.2689, 0.5921],\n",
       "        [1.2705, 0.5921],\n",
       "        [1.2718, 0.5921],\n",
       "        [1.2732, 0.5921],\n",
       "        [1.2745, 0.5921],\n",
       "        [1.2764, 0.5921],\n",
       "        [1.2793, 0.5921],\n",
       "        [1.2822, 0.5921],\n",
       "        [1.2851, 0.5921],\n",
       "        [1.2880, 0.5921],\n",
       "        [1.2901, 0.5921],\n",
       "        [1.2921, 0.5921],\n",
       "        [1.2942, 0.5921],\n",
       "        [1.2962, 0.5921],\n",
       "        [1.2985, 0.5921],\n",
       "        [1.3011, 0.5921],\n",
       "        [1.3036, 0.5921],\n",
       "        [1.3062, 0.5921],\n",
       "        [1.3086, 0.5921],\n",
       "        [1.3099, 0.5921],\n",
       "        [1.3112, 0.5921],\n",
       "        [1.3125, 0.5921],\n",
       "        [1.3138, 0.5921],\n",
       "        [1.3164, 0.5921],\n",
       "        [1.3194, 0.5921],\n",
       "        [1.3223, 0.5921],\n",
       "        [1.3252, 0.5921],\n",
       "        [1.3281, 0.5921],\n",
       "        [1.3308, 0.5921],\n",
       "        [1.3336, 0.5921],\n",
       "        [1.3364, 0.5921],\n",
       "        [1.3391, 0.5921],\n",
       "        [1.3417, 0.5921],\n",
       "        [1.3442, 0.5921],\n",
       "        [1.3467, 0.5921],\n",
       "        [1.3493, 0.5921],\n",
       "        [1.3510, 0.5921],\n",
       "        [1.3527, 0.5921],\n",
       "        [1.3544, 0.5921],\n",
       "        [1.3562, 0.5921],\n",
       "        [1.3598, 0.5921],\n",
       "        [1.3636, 0.5921],\n",
       "        [1.3675, 0.5921],\n",
       "        [1.3713, 0.5921],\n",
       "        [1.3728, 0.5921],\n",
       "        [1.3733, 0.5921],\n",
       "        [1.3739, 0.5921],\n",
       "        [1.3745, 0.5921],\n",
       "        [1.3770, 0.5921],\n",
       "        [1.3809, 0.5921],\n",
       "        [1.3847, 0.5921],\n",
       "        [1.3885, 0.5921],\n",
       "        [1.3917, 0.5921],\n",
       "        [1.3944, 0.5921],\n",
       "        [1.3972, 0.5921],\n",
       "        [1.3999, 0.5921],\n",
       "        [1.4022, 0.5921],\n",
       "        [1.4042, 0.5921],\n",
       "        [1.4063, 0.5921],\n",
       "        [1.4083, 0.5921],\n",
       "        [1.4099, 0.5921],\n",
       "        [1.4113, 0.5921],\n",
       "        [1.4126, 0.5921],\n",
       "        [1.4139, 0.5921],\n",
       "        [1.4164, 0.5921],\n",
       "        [1.4201, 0.5921],\n",
       "        [1.4238, 0.5921],\n",
       "        [1.4274, 0.5921],\n",
       "        [1.4299, 0.5921],\n",
       "        [1.4315, 0.5921],\n",
       "        [1.4331, 0.5921],\n",
       "        [1.4346, 0.5921],\n",
       "        [1.4370, 0.5921],\n",
       "        [1.4400, 0.5921],\n",
       "        [1.4429, 0.5921],\n",
       "        [1.4458, 0.5921],\n",
       "        [1.4486, 0.5921],\n",
       "        [1.4513, 0.5921],\n",
       "        [1.4540, 0.5921],\n",
       "        [1.4567, 0.5921],\n",
       "        [1.4596, 0.5921],\n",
       "        [1.4626, 0.5921],\n",
       "        [1.4656, 0.5921],\n",
       "        [1.4686, 0.5921],\n",
       "        [1.4706, 0.5921],\n",
       "        [1.4727, 0.5921],\n",
       "        [1.4747, 0.5921],\n",
       "        [1.4772, 0.5921],\n",
       "        [1.4812, 0.5921],\n",
       "        [1.4852, 0.5921],\n",
       "        [1.4892, 0.5921],\n",
       "        [1.4924, 0.5921],\n",
       "        [1.4947, 0.5921]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = diffNet(depth=10)\n",
    "\n",
    "model.forward(X)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the function with the  values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters() # accesing model parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# I need to acces the model parameters for training\n",
    "for paramA in zip(model.parameters()):\n",
    "    paramA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "constructing the loss compose function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_loss(force_params):\n",
    "    summatory = 0.0\n",
    "    for i in range(len(force_params)-1):\n",
    "        summatory += ( force_params[i+1] - force_params[i] )**2\n",
    "    return summatory\n",
    "\n",
    "def physics_constrain(force_params):\n",
    "    return force_params[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n",
      "257\n",
      "257\n",
      "257\n",
      "257\n",
      "257\n",
      "257\n",
      "257\n",
      "257\n",
      "257\n"
     ]
    }
   ],
   "source": [
    "for paramA in model.parameters():\n",
    "    print(len(paramA)) # access every force parameter at each layer\n",
    "    # but we are getting extra parameters, because it is regenerating"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remaking the diffNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class diffNet(nn.Module):\n",
    "    def __init__(self, depth):\n",
    "        super(diffNet, self).__init__()\n",
    "\n",
    "        w_mat = W_matrix(dt=1e-02)\n",
    "        f_function = F_function()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(w_mat)\n",
    "        \n",
    "        for i in range(depth):\n",
    "            layers.append( f_function )\n",
    "            layers.append( w_mat )\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return self.layers(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257\n"
     ]
    }
   ],
   "source": [
    "model = diffNet(depth=10)\n",
    "\n",
    "for paramA in model.parameters():\n",
    "    print(len(paramA)) # access every force parameter at each layer\n",
    "    # but we are getting extra parameters, because it is regenerating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
