{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import scipy.io\n",
    "import math\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras import layers, activations\n",
    "from scipy.interpolate import griddata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyDOE import lhs #latin hypercube"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PINN\n",
    "Start by analyzing a differential equation, a PDE to solve as the forward problem\n",
    "$$\n",
    "u_t + u u_x - \\frac{.01}{\\pi} u_{xx} = 0\n",
    "$$\n",
    "with $x \\in [-1,1], t \\in [0,1]$, initial condition:\n",
    "$$\n",
    "u(0,x) = - sin(\\pi x)\n",
    "$$\n",
    "and boundary condition\n",
    "$$\n",
    "u(t,-1) = u(t,1) =0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eager_lbfgs'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39meager_lbfgs\u001b[39;00m \u001b[39mimport\u001b[39;00m lbfgs, Struct\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'eager_lbfgs'"
     ]
    }
   ],
   "source": [
    "from eager_lbfgs import lbfgs, Struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "layer_sizes = [2, 20, 20, 20, 20, 20, 20, 20, 20, 1]\n",
    "\n",
    "sizes_w = []\n",
    "sizes_b = []\n",
    "for i, width in enumerate(layer_sizes):\n",
    "    if i != 1:\n",
    "        sizes_w.append(int(width * layer_sizes[1]))\n",
    "        sizes_b.append(int(width if i != 0 else layer_sizes[1]))\n",
    "\n",
    "def set_weights(model, w, sizes_w, sizes_b):\n",
    "        for i, layer in enumerate(model.layers[0:]):\n",
    "            start_weights = sum(sizes_w[:i]) + sum(sizes_b[:i])\n",
    "            end_weights = sum(sizes_w[:i+1]) + sum(sizes_b[:i])\n",
    "            weights = w[start_weights:end_weights]\n",
    "            w_div = int(sizes_w[i] / sizes_b[i])\n",
    "            weights = tf.reshape(weights, [w_div, sizes_b[i]])\n",
    "            biases = w[end_weights:end_weights + sizes_b[i]]\n",
    "            weights_biases = [weights, biases]\n",
    "            layer.set_weights(weights_biases)\n",
    "\n",
    "\n",
    "\n",
    "def get_weights(model):\n",
    "        w = []\n",
    "        for layer in model.layers[0:]:\n",
    "            weights_biases = layer.get_weights()\n",
    "            weights = weights_biases[0].flatten()\n",
    "            biases = weights_biases[1]\n",
    "            w.extend(weights)\n",
    "            w.extend(biases)\n",
    "\n",
    "        w = tf.convert_to_tensor(w)\n",
    "        return w\n",
    "\n",
    "\n",
    "def neural_net(layer_sizes):\n",
    "    model = Sequential()\n",
    "    model.add(layers.InputLayer(input_shape=(layer_sizes[0],)))\n",
    "    for width in layer_sizes[1:-1]:\n",
    "        model.add(layers.Dense(\n",
    "            width, activation=tf.nn.tanh,\n",
    "            kernel_initializer=\"glorot_normal\"))\n",
    "    model.add(layers.Dense(\n",
    "            layer_sizes[-1], activation=None,\n",
    "            kernel_initializer=\"glorot_normal\"))\n",
    "    return model\n",
    "\n",
    "\n",
    "u_model = neural_net(layer_sizes)\n",
    "\n",
    "u_model.summary()\n",
    "\n",
    "\n",
    "def loss(x_f_batch, t_f_batch,\n",
    "             x0, t0, u0, x_lb,\n",
    "             t_lb, x_ub, t_ub, col_weights, u_weights):\n",
    "\n",
    "    f_u_pred = f_model(x_f_batch, t_f_batch)\n",
    "    u0_pred = u_model(tf.concat([x0, t0],1))\n",
    "    u_lb_pred, _ = u_x_model(x_lb, t_lb)\n",
    "    u_ub_pred, _ = u_x_model(x_ub, t_ub)\n",
    "\n",
    "    mse_0_u = tf.reduce_mean(tf.square(u_weights*(u0 - u0_pred)))\n",
    "\n",
    "    mse_b_u = tf.reduce_mean(tf.square(u_lb_pred - 0)) + \\\n",
    "            tf.reduce_mean(tf.square(u_ub_pred - 0)) #since ub/lb is 0\n",
    "\n",
    "    mse_f_u = tf.reduce_mean(tf.square(col_weights*f_u_pred))\n",
    "\n",
    "\n",
    "    return  mse_0_u + mse_b_u + mse_f_u , mse_0_u, mse_f_u\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def f_model(x,t):\n",
    "    u = u_model(tf.concat([x,t], 1))\n",
    "    u_x = tf.gradients(u,x)\n",
    "    u_xx = tf.gradients(u_x, x)\n",
    "    u_t = tf.gradients(u,t)\n",
    "    f_u = u_t + u*u_x - (0.01/tf.constant(math.pi))*u_xx\n",
    "\n",
    "    return f_u\n",
    "\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def u_x_model(x,t):\n",
    "    u = u_model(tf.concat([x,t],1))\n",
    "    u_x = tf.gradients(u,x)\n",
    "    return u,u_x\n",
    "\n",
    "@tf.function\n",
    "def grad(model, x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        #tape.watch(col_weights)\n",
    "        #tape.watch(u_weights)\n",
    "        loss_value, mse_0, mse_f = loss(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "        grads = tape.gradient(loss_value, u_model.trainable_variables)\n",
    "        #print(grads)\n",
    "        grads_col = tape.gradient(loss_value, col_weights)\n",
    "        grads_u = tape.gradient(loss_value, u_weights)\n",
    "\n",
    "    return loss_value, mse_0, mse_f, grads, grads_col, grads_u\n",
    "\n",
    "def fit(x_f, t_f, x0, t0, u0, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights, tf_iter, newton_iter):\n",
    "    # Built in support for mini-batch, set to N_f (i.e. full batch) by default\n",
    "    batch_sz = N_f\n",
    "    n_batches =  N_f // batch_sz\n",
    "    start_time = time.time()\n",
    "    tf_optimizer = tf.keras.optimizers.Adam(lr = 0.005, beta_1=.90)\n",
    "    tf_optimizer_coll = tf.keras.optimizers.Adam(lr = 0.005, beta_1=.90)\n",
    "    tf_optimizer_u = tf.keras.optimizers.Adam(lr = 0.005, beta_1=.90)\n",
    "\n",
    "    print(\"starting Adam training\")\n",
    "\n",
    "    for epoch in range(tf_iter):\n",
    "        for i in range(n_batches):\n",
    "\n",
    "            x0_batch = x0#[i*batch_sz:(i*batch_sz + batch_sz),]\n",
    "            t0_batch = t0#[i*batch_sz:(i*batch_sz + batch_sz),]\n",
    "            u0_batch = u0#[i*batch_sz:(i*batch_sz + batch_sz),]\n",
    "\n",
    "            x_f_batch = x_f[i*batch_sz:(i*batch_sz + batch_sz),]\n",
    "            t_f_batch = t_f[i*batch_sz:(i*batch_sz + batch_sz),]\n",
    "\n",
    "            loss_value,mse_0, mse_f, grads, grads_col, grads_u = grad(u_model, x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "\n",
    "            tf_optimizer.apply_gradients(zip(grads, u_model.trainable_variables))\n",
    "            tf_optimizer_coll.apply_gradients(zip([-grads_col], [col_weights]))\n",
    "            tf_optimizer_u.apply_gradients(zip([-grads_u], [u_weights]))\n",
    "\n",
    "\n",
    "        if epoch % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('It: %d, Time: %.2f' % (epoch, elapsed))\n",
    "            tf.print(f\"mse_0: {mse_0}  mse_f: {mse_f}   total loss: {loss_value}\")\n",
    "            start_time = time.time()\n",
    "\n",
    "\n",
    "    #l-bfgs-b optimization\n",
    "    print(\"Starting L-BFGS training\")\n",
    "\n",
    "    loss_and_flat_grad = get_loss_and_flat_grad(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "\n",
    "    lbfgs(loss_and_flat_grad,\n",
    "      get_weights(u_model),\n",
    "      Struct(), maxIter=newton_iter, learningRate=0.8)\n",
    "\n",
    "\n",
    "# L-BFGS implementation from https://github.com/pierremtb/PINNs-TF2.0\n",
    "def get_loss_and_flat_grad(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights):\n",
    "    def loss_and_flat_grad(w):\n",
    "        with tf.GradientTape() as tape:\n",
    "            set_weights(u_model, w, sizes_w, sizes_b)\n",
    "            loss_value, _, _ = loss(x_f_batch, t_f_batch, x0_batch, t0_batch, u0_batch, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights)\n",
    "        grad = tape.gradient(loss_value, u_model.trainable_variables)\n",
    "        grad_flat = []\n",
    "        for g in grad:\n",
    "            grad_flat.append(tf.reshape(g, [-1]))\n",
    "        grad_flat = tf.concat(grad_flat, 0)\n",
    "        #print(loss_value, grad_flat)\n",
    "        return loss_value, grad_flat\n",
    "\n",
    "    return loss_and_flat_grad\n",
    "\n",
    "\n",
    "def predict(X_star):\n",
    "    X_star = tf.convert_to_tensor(X_star, dtype=tf.float32)\n",
    "    u_star, _ = u_x_model(X_star[:,0:1],\n",
    "                     X_star[:,1:2])\n",
    "\n",
    "    f_u_star = f_model(X_star[:,0:1],\n",
    "                 X_star[:,1:2])\n",
    "\n",
    "    return u_star.numpy(), f_u_star.numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "lb = np.array([-1.0]) #x upper boundary\n",
    "ub = np.array([1.0]) #x lower boundary\n",
    "\n",
    "N0 = 100\n",
    "N_b = 25 #25 per upper and lower boundary, so 50 total\n",
    "N_f = 10000\n",
    "\n",
    "col_weights = tf.Variable(tf.reshape(tf.repeat(100.0, N_f),(N_f, -1)))\n",
    "u_weights = tf.Variable(tf.random.uniform([N0, 1]))\n",
    "\n",
    "#load data, from Raissi et. al\n",
    "data = scipy.io.loadmat('burgers_shock.mat')\n",
    "\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = data['usol']\n",
    "Exact_u = np.real(Exact)\n",
    "\n",
    "\n",
    "#grab random points off the initial condition\n",
    "idx_x = np.random.choice(x.shape[0], N0, replace=False)\n",
    "x0 = x[idx_x,:]\n",
    "u0 = tf.cast(Exact_u[idx_x,0:1], dtype = tf.float32)\n",
    "\n",
    "idx_t = np.random.choice(t.shape[0], N_b, replace=False)\n",
    "tb = t[idx_t,:]\n",
    "\n",
    "# Sample collocation points via LHS\n",
    "X_f = lb + (ub-lb)*lhs(2, N_f)\n",
    "\n",
    "x_f = tf.convert_to_tensor(X_f[:,0:1], dtype=tf.float32)\n",
    "t_f = tf.convert_to_tensor(np.abs(X_f[:,1:2]), dtype=tf.float32)\n",
    "\n",
    "\n",
    "#generate point vectors for training\n",
    "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "\n",
    "#seperate point vectors\n",
    "x0 = tf.cast(X0[:,0:1], dtype = tf.float32)\n",
    "t0 = tf.cast(X0[:,1:2], dtype = tf.float32)\n",
    "\n",
    "x_lb = tf.convert_to_tensor(X_lb[:,0:1], dtype=tf.float32)\n",
    "t_lb = tf.convert_to_tensor(X_lb[:,1:2], dtype=tf.float32)\n",
    "\n",
    "x_ub = tf.convert_to_tensor(X_ub[:,0:1], dtype=tf.float32)\n",
    "t_ub = tf.convert_to_tensor(X_ub[:,1:2], dtype=tf.float32)\n",
    "\n",
    "# Begin training, modify 10000/10000 for varying levels of adam/L-BFGS respectively\n",
    "fit(x_f, t_f, x0, t0, u0, x_lb, t_lb, x_ub, t_ub, col_weights, u_weights, tf_iter = 100, newton_iter = 100)\n",
    "\n",
    "#generate mesh to find U0-pred for the whole domain\n",
    "X, T = np.meshgrid(x,t)\n",
    "\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact_u.T.flatten()[:,None]\n",
    "\n",
    "lb = np.array([-1.0, 0.0])\n",
    "ub = np.array([1.0, 1])\n",
    "\n",
    "# Get preds\n",
    "u_pred, f_u_pred = predict(X_star)\n",
    "\n",
    "#find L2 error\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "print('Error u: %e' % (error_u))\n",
    "\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "\n",
    "FU_pred = griddata(X_star, f_u_pred.flatten(), (X, T), method='cubic')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "######################################################################\n",
    "############################# Plotting ###############################\n",
    "######################################################################\n",
    "\n",
    "X0 = np.concatenate((x0, 0*x0), 1) # (x0, 0)\n",
    "X_lb = np.concatenate((0*tb + lb[0], tb), 1) # (lb[0], tb)\n",
    "X_ub = np.concatenate((0*tb + ub[0], tb), 1) # (ub[0], tb)\n",
    "X_u_train = np.vstack([X0, X_lb, X_ub])\n",
    "\n",
    "fig, ax = newfig(1.3, 1.0)\n",
    "ax.axis('off')\n",
    "\n",
    "####### Row 0: h(t,x) ##################\n",
    "gs0 = gridspec.GridSpec(1, 2)\n",
    "gs0.update(top=1-0.06, bottom=1-1/3, left=0.15, right=0.85, wspace=0)\n",
    "ax = plt.subplot(gs0[:, :])\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='YlGnBu',\n",
    "              extent=[lb[1], ub[1], lb[0], ub[0]],\n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)\n",
    "fig.colorbar(h, cax=cax)\n",
    "\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[25]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
    "ax.plot(t[50]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
    "ax.plot(t[75]*np.ones((2,1)), line, 'k--', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "leg = ax.legend(frameon=False, loc = 'best')\n",
    "#    plt.setp(leg.get_texts(), color='w')\n",
    "ax.set_title('$u(t,x)$', fontsize = 10)\n",
    "\n",
    "####### Row 1: h(t,x) slices ##################\n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1/3, bottom=0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,Exact_u[:,25], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.set_title('$t = %.2f$' % (t[25]), fontsize = 10)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1])\n",
    "ax.plot(x,Exact_u[:,50], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = %.2f$' % (t[50]), fontsize = 10)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.3), ncol=5, frameon=False)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2])\n",
    "ax.plot(x,Exact_u[:,75], 'b-', linewidth = 2, label = 'Exact')\n",
    "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = %.2f$' % (t[75]), fontsize = 10)\n",
    "\n",
    "#show u_pred across domain\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ec = plt.imshow(U_pred.T, interpolation='nearest', cmap='rainbow',\n",
    "            extent=[0.0, 1.0, -1.0, 1.0],\n",
    "            origin='lower', aspect='auto')\n",
    "\n",
    "ax.autoscale_view()\n",
    "ax.set_xlabel('$t$')\n",
    "ax.set_ylabel('$x$')\n",
    "cbar = plt.colorbar(ec)\n",
    "cbar.set_label('$u(x,t)$')\n",
    "plt.title(\"Predicted $u(x,t)$\",fontdict = {'fontsize': 14})\n",
    "plt.show()\n",
    "\n",
    "# Show F_U_pred across domain, should be close to 0\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ec = plt.imshow(FU_pred.T, interpolation='nearest', cmap='rainbow',\n",
    "            extent=[0.0, math.pi/2, -5.0, 5.0],\n",
    "            origin='lower', aspect='auto')\n",
    "\n",
    "ax.autoscale_view()\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$t$')\n",
    "cbar = plt.colorbar(ec)\n",
    "cbar.set_label('$\\overline{f}_u$ prediction')\n",
    "plt.show()\n",
    "\n",
    "# collocation point weights\n",
    "plt.scatter(t_f, x_f, c = col_weights.numpy(), s = col_weights.numpy()/5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
