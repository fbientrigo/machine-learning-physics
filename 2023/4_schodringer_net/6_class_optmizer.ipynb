{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "m = 1\n",
    "hbar = 1\n",
    "\n",
    "x0,xf = -6,6\n",
    "N = 1001\n",
    "x = torch.linspace(x0,xf,N)\n",
    "dx = (xf-x0)/N\n",
    "\n",
    "# Potential V(x)\n",
    "x_Vmin = 0        # center of V(x)\n",
    "T      = 1           # peroid of SHO \n",
    "\n",
    "omega = 2 * np.pi / T\n",
    "k = omega**2 * m\n",
    "V = torch.tensor(0.5 * k * (x - x_Vmin)**2, requires_grad = True)\n",
    "\n",
    "# V is the most important is a graded tensor, maybe delete next line if errors\n",
    "V.requires_grad_()\n",
    "V_matrix = torch.diag(V)\n",
    "V_matrix\n",
    "\n",
    "# Laplace Operator (Finite Difference)\n",
    "D2 = sparse.diags([1, -2, 1], [-1, 0, 1], shape=(N, N)) / dx**2\n",
    "\n",
    "D2.toarray()*dx**2\n",
    "\n",
    "D2t = torch.as_tensor(D2.toarray(), dtype = torch.float32)\n",
    "\n",
    "\n",
    "# Matrix of kinetic energy\n",
    "T_matrix = - (hbar**2 / (2*m)) * D2t\n",
    "H = T_matrix + V_matrix\n",
    "H\n",
    "\n",
    "# Theory\n",
    "theorical_eigvals = [hbar**2 * np.pi**2/(2*m*2)*n**2   for n in range(1,N)]\n",
    "\n",
    "limit = 100\n",
    "plt.plot(eigenvalues.detach()[:limit], label='experimental')\n",
    "plt.plot(theorical_eigvals[:limit], label='theory')\n",
    "plt.legend()\n",
    "plt.title('Eigenvalues')\n",
    "plt.show()\n",
    "\n",
    "#Comparation\n",
    "plt.plot(eigenvalues.detach().numpy()[:limit]- theorical_eigvals[:limit])\n",
    "plt.title('Error of Eigenvals')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Grow error as the first eigevalues\n",
    "limit = 100\n",
    "\n",
    "# Keep a file\n",
    "loss_array = []\n",
    "\n",
    "# Training cell epoch\n",
    "epoch = 300\n",
    "lr = 0.05\n",
    "while(epoch > 0):\n",
    "    V_matrix = torch.diag(V)\n",
    "    V_matrix\n",
    "    H = T_matrix + V_matrix\n",
    "    H\n",
    "    eigenvalues, eigenvectors = torch.linalg.eigh(H)\n",
    "    limit = 50\n",
    "    loss = L2loss(eigenvalues[:limit], target[:limit])\n",
    "    loss_array.append(loss.item())\n",
    "    V.retain_grad()\n",
    "    loss.backward()\n",
    "    V = V - lr * V.grad\n",
    "    epoch -=1\n",
    "\n",
    "# wave packet\n",
    "plt.plot(x, V.detach()*0.01, \"k--\", label=r\"evolution\")\n",
    "#plt.plot(x, np.abs(psi_wp)**2, \"r\", label=r\"$\\vert\\psi(t=0,x)\\vert^2$\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Plot\n",
    "plt.plot(loss_array[300:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The class object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class QuantumModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantumModel, self).__init__()\n",
    "        # Initialize your model layers here, including the trainable diagonal matrix.\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Implement the forward pass of your model here, including constructing the Hamiltonian matrix.\n",
    "        # You can use torch.linalg.eigh(H) to get the eigenvalues and eigenvectors.\n",
    "\n",
    "        return output\n",
    "\n",
    "    def train_model(self, train_loader, target_eigenvalues, optimizer, loss_fn, num_epochs=10):\n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, _ in train_loader:\n",
    "                # Forward pass and compute Hamiltonian matrix H\n",
    "                H = self.forward(inputs)\n",
    "\n",
    "                # Compute eigenvalues and eigenvectors\n",
    "                eigenvalues, _ = torch.linalg.eigh(H)\n",
    "\n",
    "                # Calculate the loss using the eigenvalues and target eigenvalues\n",
    "                loss = loss_fn(eigenvalues, target_eigenvalues)\n",
    "\n",
    "                # Zero gradients, backward pass, and update parameters\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Optionally print or log the loss for each epoch\n",
    "\n",
    "    def train_with_optimizer(self, train_loader, target_eigenvalues, optimizer_name, learning_rate, num_epochs=10):\n",
    "        # Choose the optimizer based on the given name and learning rate\n",
    "        if optimizer_name == \"SGD\":\n",
    "            optimizer = optim.SGD(self.parameters(), lr=learning_rate)\n",
    "        elif optimizer_name == \"Adam\":\n",
    "            optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        # Add more optimizers as needed\n",
    "\n",
    "        # Define the loss function (you may need to implement your custom loss function)\n",
    "        loss_fn = nn.MSELoss()  # Mean squared error loss as an example\n",
    "\n",
    "        # Train the model using the selected optimizer\n",
    "        self.train_model(train_loader, target_eigenvalues, optimizer, loss_fn, num_epochs)\n",
    "\n",
    "# Usage example:\n",
    "# Assuming you have defined your training data and target eigenvalues\n",
    "train_loader = torch.utils.data.DataLoader(training_data, batch_size=64)\n",
    "target_eigenvalues = torch.tensor([...])  # Your target eigenvalues\n",
    "\n",
    "# Create an instance of your QuantumModel class\n",
    "model = QuantumModel()\n",
    "\n",
    "# Train the model using the Adam optimizer with a learning rate of 2e-5 for 10 epochs\n",
    "model.train_with_optimizer(train_loader, target_eigenvalues, optimizer_name=\"Adam\", learning_rate=2e-5, num_epochs=10)\n",
    "\n",
    "# Train the model using the SGD optimizer with a learning rate of 3e-5 for 10 epochs\n",
    "model.train_with_optimizer(train_loader, target_eigenvalues, optimizer_name=\"SGD\", learning_rate=3e-5, num_epochs=10)\n",
    "\n",
    "# Add more training runs using different optimizers and learning rates as needed\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
